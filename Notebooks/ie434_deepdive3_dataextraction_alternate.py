# -*- coding: utf-8 -*-
"""IE434-DeepDive3-DataExtraction - alternate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxjBiy6YdpqqZq7W5J4R9w4H-ipf_vwM
"""

# =============================
# Data Extraction Notebook
# Project: Energy Prices in Illinois
# Milestone 1
# =============================

# Import required libraries
import pandas as pd
import requests
from io import StringIO
import pickle
import os

from google.colab import drive
drive.mount('/content/drive')

# -----------------------------
# 1. Folder Setup
# -----------------------------
BASE_PATH = "/content/drive/MyDrive/IE434-DeepDive-EnergyPricesInIllinois/Data"
RAW_PATH  = os.path.join(BASE_PATH, "raw")
PROC_PATH = os.path.join(BASE_PATH, "processed")

os.makedirs(RAW_PATH, exist_ok=True)
os.makedirs(PROC_PATH, exist_ok=True)

print("Folders ready:")
print("RAW_PATH     :", RAW_PATH)
print("PROC_PATH    :", PROC_PATH)

# -----------------------------
# 2. Define Data Sources
# -----------------------------
# Actual Energy Price (label)
ACTUAL_PRICE_URL = "http://www.energyonline.com/Data/GenericData.aspx?DataId=8&MISO___Actual_Energy_Price"

# Day Ahead Energy Price (feature)
DAY_AHEAD_PRICE_URL = "http://www.energyonline.com/Data/GenericData.aspx?DataId=8&MISO___Day_Ahead_Energy_Price"

# -----------------------------
# 3. (OPTIONAL) Inspect Tables â€“ Run Once to find correct index
# -----------------------------
"""
Uncomment and run this block *once* to see all tables and find the index that contains
the Illinois Hub price table (e.g., 8 or 9 depending on EnergyOnline layout).
"""

from io import StringIO
import pandas as pd
import requests
r = requests.get(ACTUAL_PRICE_URL)
dfs = pd.read_html(StringIO(r.text))
print(f"Found {len(dfs)} tables for Actual Energy Price")
for i, t in enumerate(dfs):
   print(f"\n--- Table {i} ---")
   print(t.head())

# After identifying the correct index, use it below (table_index=YOUR_INDEX)

# -----------------------------
# 4. Fetch and Parse Function
# -----------------------------
def fetch_energy_table(url, name="data", table_index=None):
    """
    Fetch EnergyOnline table, save raw HTML/CSV, return clean DataFrame.
    """
    print(f"\nFetching {name} from {url} ...")
    r = requests.get(url)
    if r.status_code != 200:
        raise Exception(f"Failed to fetch {name}, status {r.status_code}")

    # Save raw HTML
    raw_file = os.path.join(RAW_PATH, f"{name.lower().replace(' ','_')}.html")
    with open(raw_file, "w") as f:
        f.write(r.text)
    print(f" Saved raw HTML -> {raw_file}")

    dfs = pd.read_html(StringIO(r.text))
    print(f"Found {len(dfs)} tables for {name}")

    # Choose table
    if table_index is not None:
        df = dfs[table_index]
    else:
        # fallback to largest table
        df = max(dfs, key=lambda x: x.shape[0])

    # Clean column names
    df.columns = [str(c).strip().lower().replace(" ","_") for c in df.columns]

    # Normalize columns (Date + Value)
    if df.shape[1] >= 2:
        df.rename(
            columns={df.columns[0]: "date", df.columns[1]: f"{name.lower().replace(' ','_')}"},
            inplace=True
        )
        df["date"] = pd.to_datetime(df["date"], errors="coerce")

    # Save CSV
    csv_file = os.path.join(RAW_PATH, f"{name.lower().replace(' ','_')}.csv")
    df.to_csv(csv_file, index=False)
    print(f"ðŸ’¾ Saved CSV -> {csv_file}")
    print(df.head())

    return df

# -----------------------------
# 5. Fetch Actual + Day-Ahead Data
# -----------------------------
# Update table_index values after you run the inspection above
df_actual    = fetch_energy_table(ACTUAL_PRICE_URL, "Actual Energy Price", table_index=8)
df_day_ahead = fetch_energy_table(DAY_AHEAD_PRICE_URL, "Day Ahead Energy Price", table_index=8)

# -----------------------------
# 6. Merge & Feature Engineering
# -----------------------------
df = pd.merge(
    df_actual, df_day_ahead,
    on="date", how="inner", suffixes=("_actual", "_dayahead")
)
print(" Merged dataset shape:", df.shape)
print(df.head())

# Calendar features
df["day_of_week"] = df["date"].dt.day_name()
df["hour_of_day"] = df["date"].dt.hour
df["month"] = df["date"].dt.month

assert len(df) > 0, " Merged dataset is empty â€“ check table_index values!"

# -----------------------------
# 6. Debug & Working Datasets
# -----------------------------
df_debug = df.head(500)      # small subset for quick testing
df_working = df.copy()       # full dataset

# -----------------------------
# 7. Save Processed Datasets
# -----------------------------
df_debug   = df.head(500)
df_working = df.copy()

debug_pkl  = os.path.join(PROC_PATH, "energy_debug.pkl")
working_pkl = os.path.join(PROC_PATH, "energy_working.pkl")

df_debug.to_pickle(debug_pkl)
df_working.to_pickle(working_pkl)

print(" Saved processed debug ->", debug_pkl)
print(" Saved processed working ->", working_pkl)
print("\n Data Extraction complete! Use these PKL files in Milestone 2 notebooks.")